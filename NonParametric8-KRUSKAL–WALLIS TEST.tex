SEVERAL INDEPENDENT SAMPLES: THE KRUSKAL–WALLIS TEST
The Kruskal–Wallis test is used to test the null hypothesis that several populations have the same
medians. As such, it is the nonparametric equivalent of the one-factor completely randomized design of the
322 NONPARAMETRIC STATISTICS [CHAP. 17
analysis of variance. It is assumed that the several populations have the same form and dispersion for the
above hypothesis to be applicable, because differences in form or dispersion would also lead to rejection of
the null hypothesis. The values for the several independent random samples are required to be at least at the
ordinal scale.
The several samples are first viewed as one array of values, and each value in this combined group is
ranked from lowest to highest. For equal values the mean rank is assigned to the tied values. If the null
hypothesis is true, the average of the ranks for each sample group should be about equal. The test statistic
calculated is designated H and is based on the sum of the ranks in each of the several random samples, as
follows:

where N ¼ combined sample size of the several samples
(note that N does not designate population size in this case)
Rj ¼ sum of the ranks for the jth sample or treatment group
nj ¼ number of observations in the jth sample
Given that the size of each sample group is at least nj " 5 and the null hypothesis is true, the sampling
distribution of H is approximately distributed as the x
2 distribution with df ¼ K ! 1, where K is the number
of treatment or sample groups. The x
2 value that approximates the critical value of the test statistic is
always the upper-tail value. This test procedure is analogous to the upper tail of the F distribution being
used in the analysis of variance.
For tied ranks the test statistic H should be corrected. The corrected value of the test statistic is designated
Hc and is computed as follows:

%============================================================%
\subsection*{Methodology}
Method[edit]
Rank all data from all groups together; i.e., rank the data from 1 to N ignoring group membership. Assign any tied values the average of the ranks they would have received had they not been tied.
The test statistic is given by:
K = (N-1)\frac{\sum_{i=1}^g n_i(\bar{r}_{i\cdot} - \bar{r})^2}{\sum_{i=1}^g\sum_{j=1}^{n_i}(r_{ij} - \bar{r})^2}, where:
n_i is the number of observations in group i
r_{ij} is the rank (among all observations) of observation j from group i
N is the total number of observations across all groups
\bar{r}_{i\cdot} = \frac{\sum_{j=1}^{n_i}{r_{ij}}}{n_i},
\bar{r} =\tfrac 12 (N+1) is the average of all the r_{ij}.
If the data contain no ties the denominator of the expression for K is exactly (N-1)N(N+1)/12 and \bar{r}=\tfrac{N+1}{2}. Thus

\begin{align}
K & = \frac{12}{N(N+1)}\sum_{i=1}^g n_i \left(\bar{r}_{i\cdot} - \frac{N+1}{2}\right)^2 \\ & = \frac{12}{N(N+1)}\sum_{i=1}^g n_i \bar{r}_{i\cdot }^2 -\ 3(N+1).
\end{align}

The last formula only contains the squares of the average ranks.
A correction for ties if using the short-cut formula described in the previous point can be made by dividing K by 1 - \frac{\sum_{i=1}^G (t_i^3 - t_i)}{N^3-N}, where G is the number of groupings of different tied ranks, and ti is the number of tied values within group i that are tied at a particular value. This correction usually makes little difference in the value of K unless there are a large number of ties.
Finally, the p-value is approximated by \Pr(\chi^2_{g-1} \ge K). If some n_i values are small (i.e., less than 5) the probability distribution of K can be quite different from this chi-squared distribution. If a table of the chi-squared probability distribution is available, the critical value of chi-squared, \chi^2_{\alpha: g-1}, can be found by entering the table at g − 1 degrees of freedom and looking under the desired significance or alpha level.
If the statistic is not significant, then there is no evidence of stochastic dominance between the samples. However, if the test is significant then at least one sample stochastically dominates another sample. Therefore, a researcher might use sample contrasts between individual sample pairs, or post hoc tests using Dunn's test, which (1) properly employs the same rankings as the Kruskal-Wallis test, and (2) properly employs the pooled variance implied by the null hypothesis of the Kruskal-Wallis test in order to determine which of the sample pairs are significantly different.[4] When performing multiple sample contrasts or tests, the Type I error rate tends to become inflated, raising concerns about multiple comparisons.
