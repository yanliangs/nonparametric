
%==============================================================================================================%

Non-parametric Methods

\begin{itemize}
\item A statistical method is called non-parametric if it makes no assumption on the population distribution or sample size.

\item This is in contrast with most parametric methods in elementary statistics that assume the data is quantitative, the population has a normal distribution and the sample size is sufficiently large.

\item In general, conclusions drawn from non-parametric methods are not as powerful as the parametric ones. However, as non-parametric methods make fewer assumptions, they are more flexible, more robust, and applicable to non-quantitative data.
\end{itemize}
%==============================================================================================================%

Nonparametric statistics are statistics not based on parameterized families of probability distributions. They include both descriptive and inferential statistics. The typical parameters are the mean, variance, etc. Unlike parametric statistics, nonparametric statistics make no assumptions about the probability distributions of the variables being assessed. The difference between parametric model and non-parametric model is that the former has a fixed number of parameters, while the latter grows the number of parameters with the amount of training data.[1] Note that the non-parametric model is not none-parametric: parameters are determined by the training data, not the model.
%==============================================================================================================%
\section*{General Purpose}

Brief review of the idea of significance testing. To understand the idea of nonparametric statistics (the 
term nonparametric was first used by Wolfowitz, 1942) first requires a basic understanding of parametric 
statistics. Elementary Concepts introduces the concept of statistical significance testing based on the 
sampling distribution of a particular statistic (you may want to review that topic before reading on). 

In short, if we have a basic knowledge of the underlying distribution of a variable, then we can make 
predictions about how, in repeated samples of equal size, this particular statistic will "behave," that is, 
how it is distributed. For example, if we draw 100 random samples of 100 adults each from the general 
population, and compute the mean height in each sample, then the distribution of the standardized 
means across samples will likely approximate the normal distribution (to be precise, Student's t distribution 
with 99 degrees of freedom; see below). Now imagine that we take an additional sample in a particular 
city ("Tallburg") where we suspect that people are taller than the average population. 
If the mean height in that sample falls outside the upper 95\% tail area of the t distribution then we conclude that, indeed, the people of Tallburg are taller than the average population.

Are most variables normally distributed? In the above example we relied on our knowledge that, in repeated samples of 
equal size, the standardized means (for height) will be distributed following the t distribution (with a particular 
mean and variance). However, this will only be true if in the population the variable of interest (height in our example) 
is normally distributed, that is, if the distribution of people of particular heights follows the normal distribution 
(the bell-shape distribution).

For many variables of interest, we simply do not know for sure that this is the case. 
For example, is income distributed normally in the population? -- probably not. The incidence rates of rare diseases 
are not normally distributed in the population, the number of car accidents is also not normally distributed, and 
neither are very many other variables in which a researcher might be interested.

% For more information on the normal distribution, see Elementary Concepts; for information on tests of normality, 
% see Normality tests.

\begin{description}
\item[Sample size.] Another factor that often limits the applicability of tests based on the assumption that the sampling distribution is normal is the size of the sample of data available for the analysis (sample size; n). We can assume that the sampling distribution is normal even if we are not sure that the distribution of the variable in the population is normal, as long as our sample is large enough (e.g., 100 or more observations). However, if our sample is very small, then those tests can be used only if we are sure that the variable is normally distributed, and there is no way to test this assumption if the sample is small.

\item[Problems in measurement.] Applications of tests that are based on the normality assumptions are further limited by a lack of precise measurement. For example, let us consider a study where grade point average (GPA) is measured as the major variable of interest. Is an A average twice as good as a C average? Is the difference between a B and an A average comparable to the difference between a D and a C average? Somehow, the GPA is a crude measure of scholastic accomplishments that only allows us to establish a rank ordering of students from "good" students to "poor" students. This general measurement issue is usually discussed in statistics textbooks in terms of types of measurement or scale of measurement. Without going into too much detail, most common statistical techniques such as analysis of variance (and t- tests), regression, etc., assume that the underlying measurements are at least of interval, meaning that equally spaced intervals on the scale can be compared in a meaningful manner (e.g, B minus A is equal to D minus C). However, as in our example, this assumption is very often not tenable, and the data rather represent a rank ordering of observations (ordinal) rather than precise measurements.
\end{description}
%==================================================================%
Parametric and nonparametric methods. Hopefully, after this somewhat lengthy introduction, the need is evident for statistical procedures that enable us to process data of "low quality," from small samples, on variables about which nothing is known (concerning their distribution). Specifically, nonparametric methods were developed to be used in cases when the researcher knows nothing about the parameters of the variable of interest in the population (hence the name nonparametric). In more technical terms, nonparametric methods do not rely on the estimation of parameters (such as the mean or the standard deviation) describing the distribution of the variable of interest in the population. Therefore, these methods are also sometimes (and more appropriately) called parameter-free methods or distribution-free methods.
%=========================================================================================================%

Nonparametric statistics are statistics not based on parameterized families of probability distributions. 
They include both descriptive and inferential statistics. The typical parameters are the mean, variance, etc. Unlike parametric statistics, nonparametric statistics make no assumptions about the probability distributions of the variables being assessed. The difference between parametric model and non-parametric model is that the former has a fixed number of parameters, while the latter grows the number of parameters with the amount of training data.[1] Note that the non-parametric model is not none-parametric.


%=====================================================%
% Nonparametric Statistics
\subsection{17.1 SCALES OF MEASUREMENT}
Before considering how nonparametric methods of statistics differ from the parametric procedures that
constitute most of this book, it is useful to define four types of measurement scales in terms of the precision
represented by reported values.
In the nominal scale, numbers are used only to identify categories. They do not represent any amount or
quantity as such.
\begin{description}
\item[EXAMPLE 1.] If four sales regions are numbered 1 through 4 as general identification numbers only, then the nominal
scale is involved, since the numbers simply serve as category names.
In the ordinal scale, the numbers represent ranks. The numbers indicate relative magnitude, but the
differences between the ranks are not assumed to be equal.
\item[EXAMPLE 2.] An investment analyst ranks five stocks from 1 to 5 in terms of appreciation potential. The difference in the
appreciation potential between the stocks ranked 1 and 2 generally would not be the same as, say, the difference between the
stocks ranked 3 and 4.
In the interval scale, measured differences between values are represented. However, the zero point is
arbitrary, and is not an “absolute” zero. Therefore, the numbers cannot be compared by ratios.
\item[EXAMPLE 3.] In either the Fahrenheit or the Celsius temperature scales, a 58 difference, from, say, 708F, to 758F is the
same amount of difference in temperature as from 808F to 858F. However, we cannot say that 608F is twice as warm as 308F,
because the 08F point is not an absolute zero point (the complete absence of all heat).
\end{description}
%=====================================================%
